{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycaret\n",
    "!pip install python-dotenv\n",
    "!pip install ydata-profiling\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Introduction </h3>\n",
    "This template notebook will give users (data scientists and data engineers) the opportunities to train and deploy regression machine learning models with ease, without having to write all of the code from scratch. <br> \n",
    "There are a few requirements for the user anyway, such as;\n",
    "<li>the data location</li> \n",
    "<li>algorithm_choice: what type of machine learning model you are trying to build. Regression, classification, time series etc.</li>\n",
    "<li>the target (dependent) variable in your dataset and</li>\n",
    "<li>endpoint_name: what you will like to call your endpoint after it is created</li>\n",
    "<li>model_name: what you will like to call your model</li>\n",
    "<li>data_location: the location of the datasets. Needs to be in an S3 bucket.</li>\n",
    "<li>pycaret_ecr_name: what you have named the ecr image that you created </li>\n",
    "<li>instance_type: the details of the instance type that you will be creating. </li>\n",
    "\n",
    "<b>All of these values will be entered in terraform and will be automatically applied in the notebook.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Imports</h3>\n",
    "\n",
    "The libraries that are required for this model notebook are imported below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "from dotenv import load_dotenv\n",
    "from load_data import load_data\n",
    "from split_data import split_data\n",
    "import importlib\n",
    "from save_model_to_s3 import save_model_to_s3\n",
    "from deploy_model_endpoint import deploy_model\n",
    "from finalize_and_save_model import finalize_and_save_model\n",
    "from delete_sagemaker_endpoint import delete_sagemaker_endpoint\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Loading Data</h3>\n",
    "Here the user is required to specify the location of the data that they will like to use for prediction. An helper function is used to load the data from S3. \n",
    "\n",
    "<em>Note: Your data needs to be an s3 bucket.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables Setup Stage\n",
    "load_dotenv(\".env\")\n",
    "role = get_execution_role()\n",
    "\n",
    "# Env variables\n",
    "data_location_s3 = os.getenv(\"data_location_s3\")\n",
    "algorithm_choice = os.getenv(\"algorithm_choice\")\n",
    "target = os.getenv(\"target\")\n",
    "endpoint_name = os.getenv(\"endpoint_name\")\n",
    "model_name = os.getenv(\"model_name\")\n",
    "data_location = 's3://{}'.format(data_location_s3)\n",
    "instance_type = os.getenv(\"instance_type\")\n",
    "model_instance_count = int(os.getenv(\"model_instance_count\"))\n",
    "image_uri = os.getenv(\"ecr_repo_uri\")\n",
    "tuning_metric = os.getenv(\"tuning_metric\")\n",
    "\n",
    "print(data_location_s3, algorithm_choice, target, endpoint_name, model_name, data_location, instance_type, image_uri, tuning_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_role = (\n",
    "    \"arn:aws:iam::135544376709:role/banking-classification-query_training_status-role\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Read and display a sample of data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from S3\n",
    "df = load_data(data_location)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Data Exploration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(\n",
    "    df,\n",
    "    sort=None,\n",
    "    html={\"style\": {\"full_width\": True}},\n",
    "    title=\"Data Exploration\",\n",
    "    explorative=True,\n",
    ")\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and shuffle data\n",
    "train_data, test_data = split_data(df, shuffle=True)\n",
    "print(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Data Cleaning and Feature Engineer</h3> \n",
    "\n",
    "Here we work on Data Cleaning and Feature Engineering before passing data down to pycaret's environment. This feature is still in progress.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBC (Just a Placeholder currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pycaret library depending on the algorithm choice\n",
    "pycaret = importlib.import_module(f\"pycaret.{algorithm_choice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data in PyCaret with all the defined parameters\n",
    "pycaret.setup(data=train_data, target=target, session_id=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7.Model Training</h3>\n",
    "\n",
    "Here we are using the pycaret automl tool to train the model. The automl tool tries a number of machine learning algorithms depending on the type of machine learning problem you are trying to solve <br>\n",
    "(regression, classification or time series). The automl tool then selects the best model based on the accuracy metrics of the tried models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the performance of all estimators available in the model library using cross-validation.\n",
    "bestModel = pycaret.compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7.5. Hyperparameter Tuning: </h3>\n",
    "\n",
    "Here we optimize the model using hyperparameter tuning. This can be done by using pycaret automatic hyperparameter tuning function, or choosing the metric user want to focus on optimizing eg. MAE, RMSE for regression, and Accuracy, AUC for classification. User can also specify how many iteration user want to run (default is 10 iterations).\n",
    "\n",
    "This can be done by using these commands\n",
    "```\n",
    "pycaret.tune_model(model)\n",
    "pycaret.tune_model(model, n_iter = <number of iteration>)\n",
    "pycaret.tune_model(model, optimize = '<Metric>')\n",
    "```\n",
    "\n",
    "In the case where original model has a better performance, pycaret will return the original model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = pycaret.tune_model(bestModel, n_iter = 10, optimize = tuning_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model's hyperparameter\n",
    "print(bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. Model Evaluation: </h3>\n",
    "\n",
    "Here we evaluate the performance of the best model, getting some visual representation of hyperparameters, features and other important details about the selected model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model: Display UI analyzing Hyperparameters, Confusion Matrix, Class Report, etc.\n",
    "pycaret.evaluate_model(bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>9. Saving Model for future predition</h3>\n",
    "\n",
    "Here we are using a function that saves the model to s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finalising model and save the model to current directory\n",
    "final_model = finalize_and_save_model(algorithm_choice, bestModel, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = pycaret.predict_model(final_model, data=test_data)\n",
    "final_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to s3\n",
    "save_model_to_s3(model_name, f'{model_name}-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model explainability functions:</h3>\n",
    "\n",
    "This visualization is designed to explain the output of a single prediction by showing the contribution of each feature to the final prediction, compared to a base value.\n",
    "\n",
    "\n",
    "<li><b>Base Value:</b> The base value is the reference point for the model's predictions. It's the value that would be predicted if we didn't know any features for the current output. In SHAP, this is typically the mean prediction of the model over the training dataset.</li>\n",
    "<li><b>Output Value (f(x)):</b> This is the actual prediction for the instance being explained. This value is the sum of the base value and all of the SHAP values (feature contributions) for this prediction.</li>\n",
    "<li><b>Red and Blue Arrows:</b> Each arrow represents a feature that contributes to the prediction. Features pushing the prediction higher (toward the right) are shown in red, and those pushing the prediction lower (toward the left) are shown in blue. The length of each arrow represents the magnitude of the feature's contribution.</li>\n",
    "<li><b>Red:</b> The feature increases the model's prediction. Pushes the prediction to a higher value, indicating that this feature has a positive impact on the prediction.</li>\n",
    "<li><b>Blue:</b> If there were any, blue would indicate features that push the model's prediction to a lower value.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "test_features = test_data.copy().drop(target, axis=1)\n",
    "\n",
    "explainer = shap.Explainer(final_model.predict, test_features)\n",
    "\n",
    "# Generate SHAP values for your test features\n",
    "shap_values = explainer(test_features)\n",
    "\n",
    "base_value = shap_values.base_values[0]  # This is for a single output model\n",
    "\n",
    "shap_values_array = shap_values.values[0]\n",
    "\n",
    "# Visualize the first prediction's explanation\n",
    "shap.initjs()\n",
    "shap.force_plot(base_value, shap_values_array, test_features.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap Summary Plot\n",
    "We will use summary_plot. This type of plot aggregates SHAP values for all the features and all samples in the selected set. Then SHAP values are sorted, so the first one shown is the most important feature. In addition to that, we are provided with information of how each feature affects the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_array = shap_values.values\n",
    "\n",
    "# Plotting the summary plot for all features across all test samples\n",
    "shap.summary_plot(shap_values_array, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>10. Deploying the model endpoints</h3> \n",
    "\n",
    "Here we use a function that creates the model endpoint in sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to sagemaker endpoint\n",
    "deploy_model(model_name, instance_type, endpoint_name, role, model_instance_count, image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up stage\n",
    "## Remove Endpoint and Endpoint Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_sagemaker_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
